"""
é€šè¿‡APIè·å–å½“å‰é…ç½®çš„å®é™…å·¥ä½œæµJSON
"""
import requests
import json
import sys
sys.path.insert(0, '..')
from config import API_KEY, BASE_URL

def get_workflow_json(workflow_id):
    """é€šè¿‡APIè·å–å·¥ä½œæµJSON"""
    url = f"{BASE_URL}/api/openapi/getJsonApiFormat"
    headers = {
        "Host": "www.runninghub.cn",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    payload = {
        "apiKey": API_KEY,
        "workflowId": workflow_id
    }

    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        result = resp.json()

        if result.get('code') == 0:
            prompt_data = result.get('data', {}).get('prompt', '{}')
            if isinstance(prompt_data, str):
                return json.loads(prompt_data)
            return prompt_data
        else:
            print(f"âŒ è·å–å·¥ä½œæµ {workflow_id} å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
            return None
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return None

def analyze_watermark_removal(workflow_json, workflow_name):
    """åˆ†æå»æ°´å°åŸç†"""
    print(f"\n{'='*80}")
    print(f"ã€{workflow_name}ã€‘å»æ°´å°åŸç†åˆ†æ")
    print(f"{'='*80}")

    # 1. æ‰¾å‡ºæ‰€æœ‰å…³é”®èŠ‚ç‚¹
    input_nodes = []  # è¾“å…¥èŠ‚ç‚¹
    output_nodes = []  # è¾“å‡ºèŠ‚ç‚¹
    ai_model_nodes = []  # AIæ¨¡å‹èŠ‚ç‚¹
    processing_nodes = []  # å¤„ç†èŠ‚ç‚¹

    for node_id, node_data in workflow_json.items():
        class_type = node_data.get('class_type', '')
        title = node_data.get('_meta', {}).get('title', '')

        # è¾“å…¥èŠ‚ç‚¹
        if class_type in ['LoadImage', 'LoadVideo', 'VHS_LoadVideo']:
            input_nodes.append((node_id, class_type, title, node_data.get('inputs', {}).keys()))

        # è¾“å‡ºèŠ‚ç‚¹
        elif class_type in ['SaveImage', 'SaveVideo']:
            output_nodes.append((node_id, class_type, title))

        # AIæ¨¡å‹èŠ‚ç‚¹
        elif class_type in ['UNETLoader', 'VAELoader', 'CLIPLoader']:
            model_name = node_data.get('inputs', {}).get('unet_name') or \
                        node_data.get('inputs', {}).get('vae_name') or \
                        node_data.get('inputs', {}).get('clip_name', '')
            ai_model_nodes.append((node_id, class_type, title, model_name))

        # é‡‡æ ·/ç”ŸæˆèŠ‚ç‚¹
        elif 'Sampler' in class_type or class_type in ['KSampler', 'SamplerCustomAdvanced']:
            processing_nodes.append((node_id, class_type, title))

        # ç¼–ç /è§£ç èŠ‚ç‚¹
        elif 'VAEEncode' in class_type or 'VAEDecode' in class_type:
            processing_nodes.append((node_id, class_type, title))

        # å‚è€ƒ latent èŠ‚ç‚¹ï¼ˆé£æ ¼è¿ç§»å…³é”®ï¼‰
        elif 'ReferenceLatent' in class_type:
            processing_nodes.append((node_id, class_type, title + " âš ï¸é£æ ¼å‚è€ƒ"))

        # æ–‡æœ¬ç¼–ç ï¼ˆæç¤ºè¯ï¼‰
        elif 'CLIPTextEncode' in class_type or 'JjkText' in class_type:
            text = node_data.get('inputs', {}).get('text', '')
            if isinstance(text, str) and len(text) > 50:
                processing_nodes.append((node_id, class_type, title[:50] + "..."))

    # æ‰“å°åˆ†æç»“æœ
    print("\nğŸ“¥ è¾“å…¥èŠ‚ç‚¹:")
    for node in input_nodes:
        print(f"   èŠ‚ç‚¹ {node[0]}: {node[1]} - {node[2]}")
        print(f"      å­—æ®µ: {list(node[3])}")

    print("\n